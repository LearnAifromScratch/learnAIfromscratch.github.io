<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-10-16 Wed 20:21 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>AI in 2024</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="jbh" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<style> body {background-color: #fafad2; max-width: 62.5rem; padding: 1rem; margin: auto;} </style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2020 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">AI in 2024</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org757f6f2">Intro</a>
<ul>
<li><a href="#org00057a7">Limits of AI</a></li>
</ul>
</li>
<li><a href="#org8b67a3b">PreReqs</a>
<ul>
<li><a href="#orged5a6d0">Basics of scalar calculus</a></li>
<li><a href="#orgbafea7f">Basics of vector calculus</a></li>
<li><a href="#orga3956ac">Matrix calculus</a></li>
<li><a href="#orgb1b78bd">Probability &amp; Statistics</a></li>
<li><a href="#orgca978c6">LADR (TODO)</a></li>
</ul>
</li>
</ul>
</div>
</div>
<style>details summary { color: green; }</style>

<div id="outline-container-org757f6f2" class="outline-2">
<h2 id="org757f6f2">Intro</h2>
<div class="outline-text-2" id="text-org757f6f2">
<p>
AI models are almost affordable for us to play around with as the GPU bubble finally <a href="https://www.latent.space/p/gpu-bubble">crashed</a> but it's still prohibitively expensive to lease or own anything reliable GPU pods so in the meanwhile there's a new field called tranformer <a href="https://www.neelnanda.io/mechanistic-interpretability/glossary">Mechanistic Interpretability</a> (MI) we can try which is essentially reverse engineering transformer architecture like LLMs. 
</p>

<p>
Neel Nanda has helpfully produced a concrete <a href="https://www.neelnanda.io/mechanistic-interpretability/getting-started">curriculum</a> for this and even offers <a href="https://www.matsprogram.org/interpretability">mentorship</a> through the ML Alignment &amp; Theory Scholars program so we may as well start there. 
</p>
</div>

<div id="outline-container-org00057a7" class="outline-3">
<h3 id="org00057a7">Limits of AI</h3>
<div class="outline-text-3" id="text-org00057a7">
<p>
Current popular AI is all <a href="https://en.wikipedia.org/wiki/Foundation_model">foundation models</a> like GPT-n, DALL-E etc. Have you wondered if we had infinite resources, infinite data, perfect training algorithms with no errors, can we use this type of model for everything? Someone with help from the Beijing Academy of AI used <a href="https://proceedings.mlr.press/v202/yuan23b.html">category theory</a> to model this scenario to see what is possible.
</p>
</div>
</div>
</div>

<div id="outline-container-org8b67a3b" class="outline-2">
<h2 id="org8b67a3b">PreReqs</h2>
<div class="outline-text-2" id="text-org8b67a3b">
<p>
Looking at his <a href="https://www.neelnanda.io/mechanistic-interpretability/prereqs">guide</a> for prereqs, he suggests to learn the essentials of ML/Backpropagation and we have access to:
</p>

<ul class="org-ul">
<li><a href="https://jorchard.github.io/cs479.github.io/index.html">CS 479 Neural Networks</a> is an interesting Waterloo survey on neural networks from the perspective of theoretical neuroscience (Backpropagation, gradients/chain rule, basics of ML).</li>
<li><a href="https://deeplearning.cs.cmu.edu/F24/index.html">11-785 Intro to Deep Learning</a> is CMU's premiere DL course that covers some new architectures.</li>
<li><a href="https://dlsyscourse.org/lectures/">10-414 Deep Learning Systems</a> is CMU's mostly open course about the libraries of PyTorch and TensorFlow.</li>
</ul>

<p>
I'll probably just audit CS 479 lectures as we only need the bare minimum requirements here all the hard work will come later tearing apart LLMs.
</p>
</div>

<div id="outline-container-orged5a6d0" class="outline-3">
<h3 id="orged5a6d0">Basics of scalar calculus</h3>
<div class="outline-text-3" id="text-orged5a6d0">
<p>
The best crash course I know of is <a href="https://www.youtube.com/watch?v=zfHEkt-1sqo">this</a> video by Wildberger. 
</p>

<p>
@18:40 try your own examples of the derivative function plugging in common functions. Normally in symbolic calculus you have to finess with algebra rewrites until you eliminate h or get to a point where you can start replacing h with the limit 0. A derivative of a function always gives you a second function.
</p>

<p>
Try the derivative of f(x) = x<sup>2</sup>
\[\frac{(a + h)^2 - a^2}{h}\] 
\[= \frac{(a + h)(a + h) - a^2}{h}\]
\[= \frac{a^2 + 2ah + h^2 - a^2}{h}\]
\[= \frac{2ah}{h} + \frac{h^2}{h}\]
\[= 2a + h\]
\[\lim_{h \to 0} 2a + 0\]
\[f'(a) = 2a\]
</p>

<p>
Take the derivative of f(x) = 2x (higher-order derivative)
</p>

<p>
\[\frac{2(a + h) - 2a}{h}\] 
\[= \frac{2a + 2h- 2a}{h}\]
\[= \frac{2h}{h}\]
\[f''(a) = 2\]
</p>

<p>
Take the derivative of f(x) = 2 or 2x<sup>0</sup>
</p>

<p>
\[\frac{2(a + h)^0 - 2a^0}{h}\] 
\[= \frac{2 - 2}{h}\]
\[= \frac{0}{h}\]
\[f'''(x) = 0\]
</p>

<p>
This makes sense if we have a constant function f(x) = c producing a straight horizontal line that for every value f(x) outputs the same y then it's slope is 0. @25:24 note the simple chain rule if y = g(x) then finding the derivative of a composite function f(g(x)) using the chain rule is: f'(y) * g'(x) and if you're interested 3blue1brown has a visualization <a href="https://www.youtube.com/watch?v=YG15m2VwSjA">here</a> but we will see all this later in higher dimensions generalized. 
</p>

<p>
At the end of this he shows how indefinite integration of entire functions is the undo method for derivatives and that's all you need to know about introductory calculus. When we come across positional encoding matrices (sine/cosine) while learning transformers we will pick that up there as needed. 
</p>
</div>
</div>

<div id="outline-container-orgbafea7f" class="outline-3">
<h3 id="orgbafea7f">Basics of vector calculus</h3>
<div class="outline-text-3" id="text-orgbafea7f">
<p>
OCW has a <a href="https://ocw.mit.edu/courses/18-02sc-multivariable-calculus-fall-2010/">free course</a> we need to cherry pick the basics of vector spaces in 3D, chain rule, gradient and double integrals for probability. 
</p>
</div>
</div>

<div id="outline-container-orga3956ac" class="outline-3">
<h3 id="orga3956ac">Matrix calculus</h3>
<div class="outline-text-3" id="text-orga3956ac">
<p>
We probably need to see a few lectures from MIT's <a href="https://ocw.mit.edu/courses/18-s096-matrix-calculus-for-machine-learning-and-beyond-january-iap-2023/">Matrix Calculus</a> short course which is derivatives generalized to higher dimensions, 'matrix gradients' beyond column vectors, and Autodiff of computational graphs (a neural network is just a graph).
</p>
</div>
</div>

<div id="outline-container-orgb1b78bd" class="outline-3">
<h3 id="orgb1b78bd">Probability &amp; Statistics</h3>
<div class="outline-text-3" id="text-orgb1b78bd">
<p>
We need a few chapters from <a href="https://www.cs.cmu.edu/~harchol/Probability/book.html">here</a> a Q&amp;A style workbook.
</p>
</div>
</div>

<div id="outline-container-orgca978c6" class="outline-3">
<h3 id="orgca978c6">LADR (TODO)</h3>
<div class="outline-text-3" id="text-orgca978c6">
<p>
Axler's free <a href="https://linear.axler.net/">book</a> which we can complete while continuing the <a href="https://www.neelnanda.io/mechanistic-interpretability/getting-started">transformer</a> curriculum where the goal is to analyse a model up to GPT-2 in size by reverse engineering a trained model to re-derive its positional encoding mappings. Edelman's <a href="https://github.com/mitmath/1806/blob/spring20/summaries.md">version</a> of 18.06 uses Julia notebooks and is modernized throwing away echelon forms, free variables and pivots, and even representations of large matrices as tables of numbers. We can use it with Axler's book.   
</p>

<hr />
<p>
<a href="./index.html">Home</a>
</p>
</div>
</div>
</div>
</div>
</body>
</html>
