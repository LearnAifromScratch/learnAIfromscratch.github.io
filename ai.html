<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2025-07-27 Sun 16:22 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>AI in 2025</title>
<meta name="author" content="jbh" />
<meta name="generator" content="Org Mode" />
<style type="text/css">
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<style> body {background-color: #fafad2; max-width: 62.5rem; padding: 1rem; margin: auto;} </style>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">AI in 2025</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org9101f39">Intro</a></li>
<li><a href="#org19848d9">Curriculum</a>
<ul>
<li><a href="#orgc48cb87">What we want to achieve</a></li>
<li><a href="#org0f9c713">How to get there</a>
<ul>
<li><a href="#orgaceab69">Intro to machine learning</a></li>
<li><a href="#orge119de8">Neural Networks</a></li>
</ul>
</li>
<li><a href="#org236015c">Math we need</a>
<ul>
<li><a href="#org296e837">Linear Algebra</a></li>
<li><a href="#org43ef110">Calculus</a></li>
<li><a href="#org34d3142">Statistics</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgd804f32">Day 1 Neuroscience Models</a></li>
<li><a href="#orgfeee97f">Day 2 Calculus</a>
<ul>
<li><a href="#org7725011">Derivatives</a></li>
<li><a href="#org24d4257">Limits</a>
<ul>
<li><a href="#org64c0686">Trig functions</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org1cf291d">Day 3 Integrals</a></li>
</ul>
</div>
</div>
<div id="outline-container-org9101f39" class="outline-2">
<h2 id="org9101f39">Intro</h2>
<div class="outline-text-2" id="text-org9101f39">
<p>
We will learn the 'full stack' of modern deep learning systems by building our own library from scratch. Neural networks have different architectures just like there is different computer hardware architectures (x86, RISC, ARM). The most popular currently is transformer architecture for large language models (Grok-n, GPT-n).   
</p>

<p>
<a href="https://www.neelnanda.io/about">Neel Nanda</a> has helpfully produced a suggested <a href="https://www.neelnanda.io/mechanistic-interpretability/getting-started">curriculum</a> for reverse engineering transformers. He even offers <a href="https://www.matsprogram.org/interpretability">mentorship</a> through the ML Alignment &amp; Theory Scholars program or you can claim bounties for GeoHot's <a href="https://docs.google.com/spreadsheets/d/1WKHbT-7KOgjEawq5h5Ic1qUWzpfAzuD_J06N1JwOCGs/edit?gid=0#gid=0">TinyGrad</a> as a freelancer.
</p>


<p>
<b>Limits of AI</b>
</p>

<p>
Current popular AI is all <a href="https://en.wikipedia.org/wiki/Foundation_model">foundation models</a> like Grok-n, GPT-n, DALL-E etc. Have you wondered if we had infinite resources, infinite data, perfect training algorithms with no errors (aka ideal models), can we use this type of model for everything aka 'General AI'? Someone with help from the Beijing Academy of AI used <a href="https://proceedings.mlr.press/v202/yuan23b.html">category theory</a> to model this scenario to see what is possible. The multimodal content here is interesting. 
</p>
</div>
</div>
<div id="outline-container-org19848d9" class="outline-2">
<h2 id="org19848d9">Curriculum</h2>
<div class="outline-text-2" id="text-org19848d9">
<p>
Most of the work is programming in (free) google colab notebooks in C++ and Python though since we are building everything from scratch you could use the language of your choice. Someone wrote Llama.cpp why not write your own too.
</p>
</div>
<div id="outline-container-orgc48cb87" class="outline-3">
<h3 id="orgc48cb87">What we want to achieve</h3>
<div class="outline-text-3" id="text-orgc48cb87">
<ul class="org-ul">
<li><a href="https://jorchard.github.io/cs479.github.io/index.html">CS 479 Neural Networks</a> (Waterloo)
<ul class="org-ul">
<li>Survey of modern NN from the perspective of theoretical neuroscience</li>
<li>We also take <a href="https://youtu.be/VMj-3S1tku0?si=1xkNo1BQ1Y_FcjbS">Building Micrograd</a> by Andrej Karpathy</li>
</ul></li>
<li><a href="https://dlsyscourse.org">10-714 DL Algorithms &amp; Implementation</a> (CMU)
<ul class="org-ul">
<li>Build from scratch transformers, RNNs, MLPs, everything</li>
<li>Includes hardware acceleration</li>
</ul></li>
<li><a href="https://www.neelnanda.io/mechanistic-interpretability/getting-started">Mechanistic Interpretability</a> (Neel Nanda@Google DeepMind)
<ul class="org-ul">
<li>Reverse engineering transformers</li>
<li>Core material is <a href="https://github.com/callummcdougall/ARENA_3.0/tree/main">here</a> and 90% programming
<ul class="org-ul">
<li>Chapter <a href="https://arena-chapter0-fundamentals.streamlit.app/">0</a></li>
<li>Chapter <a href="https://arena-chapter1-transformer-interp.streamlit.app/">1</a></li>
<li>Chapter <a href="https://arena-chapter2-rl.streamlit.app/">2</a></li>
<li>Chapter <a href="https://arena-chapter3-llm-evals.streamlit.app/">3</a></li>
</ul></li>
</ul></li>
</ul>

<p>
<b>Research</b>
</p>

<p>
This is a graduate course on trying to integrate everything into a shared reasoning and represenation system (text, audio, video, actions) so it's the final boss of the AI game. Multimodal doesn't require enormous pretraining such as transformer-based models and can overcome data scarcity. This means we as pleb researchers can modify open source models like this <a href="https://github.com/Open-Finance-Lab/Awesome-MFFMs/">finance</a> multimodal foundation model. 
</p>

<ul class="org-ul">
<li><a href="https://cmu-mmml.github.io/">Multimodal Machine Learning</a> (CMU)
<ul class="org-ul">
<li>We have access to 2023 <a href="https://www.youtube.com/watch?v=DPkwjgaRvyI&amp;list=PL-Fhd_vrvisMYs8A5j7sj8YW1wHhoJSmW&amp;index=1">lectures</a> but recent lecture structure remains the same</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org0f9c713" class="outline-3">
<h3 id="org0f9c713">How to get there</h3>
<div class="outline-text-3" id="text-org0f9c713">
</div>
<div id="outline-container-orgaceab69" class="outline-4">
<h4 id="orgaceab69">Intro to machine learning</h4>
<div class="outline-text-4" id="text-orgaceab69">
<p>
Every school has an undergrad introduction to machine learning and these courses don't really change from year to year as the underlying concepts to the theory remain the same.
See CMU's latest <a href="https://www.cs.cmu.edu/~mgormley/courses/10601/schedule.html">Intro to Machine Learning</a> (2025) and compare that with the chapters from this 2014 <a href="https://www.cs.cmu.edu/~nihars/teaching/10715-Fa23/index.html">book</a> on the theory of ML. K-nearest neighbors, perceptron (linear predictors), gradient descent, feature engineering, optimization (convex and non-convex), decision trees, PAC learning (probably approximately correct), loss functions, boosting, random forests, dimensionality reduction, it's all in that 2014 book because that is the mathematical model of machine learning. If you want the CMU undergrad version you can go back in time using any university's <a href="https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%22892c28d3-f548-4a7a-9de0-ae90011552fa%22">panopto</a> (Summer 2022) recordings. 
</p>

<p>
The <a href="https://www.cs.cmu.edu/~nihars/teaching/10715-Fa23/index.html">graduate</a> versions of introductory machine learning still use that 2014 book so we may as well. Lucky for us the author of that book made a series of lectures for his course at Waterloo and it's a very unusual course he walks through all the math assuming little background.     
</p>

<ul class="org-ul">
<li><a href="https://student.cs.uwaterloo.ca/~cs485/">CS 485 Theory of ML</a> (Waterloo)   
<ul class="org-ul">
<li>All the <a href="https://www.youtube.com/playlist?list=PLt6ES1TJ1gtsvVE_jD-nYaxB2yJzk4zNB">lectures</a> on YouTube</li>
<li>Complete mathematical walkthrough of ML and algorithms</li>
<li>Timeless material as core theory hasn't changed</li>
<li>Matches with Kilian Weinberger's <a href="https://www.youtube.com/playlist?list=PLl8OlHZGYOQ7bkVbuRthEsaLr7bONzbXS">lectures</a> too</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orge119de8" class="outline-4">
<h4 id="orge119de8">Neural Networks</h4>
<div class="outline-text-4" id="text-orge119de8">
<p>
This is to understand the neural networks in the interpretability courses we do and the CMU course on building our own libraries. 
</p>

<ul class="org-ul">
<li><a href="https://jorchard.github.io/cs479.github.io/index.html">CS 479 Neural Networks</a> (Waterloo)
<ul class="org-ul">
<li>Survey of modern NN from the perspective of theoretical neuroscience</li>
<li>We also take <a href="https://youtu.be/VMj-3S1tku0?si=1xkNo1BQ1Y_FcjbS">Building Micrograd</a> by Andrej Karpathy</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org236015c" class="outline-3">
<h3 id="org236015c">Math we need</h3>
<div class="outline-text-3" id="text-org236015c">
<p>
These can all be done in parallel 
</p>
</div>
<div id="outline-container-org296e837" class="outline-4">
<h4 id="org296e837">Linear Algebra</h4>
<div class="outline-text-4" id="text-org296e837">
<ul class="org-ul">
<li><a href="https://cs.brown.edu/courses/cs053/current/lectures.htm">Coding the Matrix</a> (Brown)
<ul class="org-ul">
<li>All the recorded lectures are open to anyone</li>
<li>Programmed w/Python and has a (nonfree) <a href="https://codingthematrix.com/">book</a></li>
</ul></li>
</ul>

<p>
Created by Philip Klein a name you will recognize if you take any algorithms course. Probability and the SVD is covered which we'll need. The book is at least cheap ($35) and worth buying or you can use <a href="https://annas-archive.org/slow_download/13d7370d8f4ea2d2c3885be0a21bc01e/0/0">Anna's Archive</a> or whatever the <a href="https://open-slum.org/">latest</a> Library Genesis domain is to get a pdf.
</p>

<p>
Neel Nanda who provided the reverse engineering course suggested we take Axler's <a href="https://linear.axler.net/">LADR</a> and the new 4th version is totally free (he even upgraded his cat in the about author pic). If you look at the table of contents for both Brown's <i>Coding the Matrix</i> and LADR they match up almost perfectly so we can take both at the same time and/or review LADR when needed.
</p>
</div>
</div>
<div id="outline-container-org43ef110" class="outline-4">
<h4 id="org43ef110">Calculus</h4>
<div class="outline-text-4" id="text-org43ef110">
<p>
We will take the world's shortest shortcut to learn basic calculus then fill in all the blanks with matrix calculus.  
</p>

<ul class="org-ul">
<li><a href="https://ocw.mit.edu/courses/18-s096-matrix-calculus-for-machine-learning-and-beyond-january-iap-2023/">Matrix Calculus</a> (MIT)
<ul class="org-ul">
<li>All the calculus we need generalized to higher dimensions</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org34d3142" class="outline-4">
<h4 id="org34d3142">Statistics</h4>
<div class="outline-text-4" id="text-org34d3142">
<p>
A free <a href="https://www.cs.cmu.edu/~harchol/Probability/book.html">workbook</a> on probability/statistics that we can take with the probability content in <i>Coding the Matrix</i> and the stats content in <i>Understanding Machine Learning</i>.
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orgd804f32" class="outline-2">
<h2 id="orgd804f32">Day 1 Neuroscience Models</h2>
<div class="outline-text-2" id="text-orgd804f32">
<p>
Reminder this is 'learn AI from scratch' thus here is neural models from scratch. 
</p>

<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=uBHW5Rs7qsA&amp;feature=youtu.be">What is a Neuron</a> (26m) from <a href="https://jorchard.github.io/cs479.github.io/index.html">CS479</a>.
<ul class="org-ul">
<li>Hodgkin-Huxley Neuron Model</li>
</ul></li>
</ul>

<p>
@6:39 his example of an action potential using a water bucket being filled and dumped is exactly the same intuition of a potential function in a data structures analysis course used for amortized analysis. A dynamic array once filled up to a certain percentage needs to perform a costly action of doubling itself to enable more free space. You take the infrequent cost of doubling and average it over many read/writes to come up with an individual cost of all array operations. There's some differential equations here but he draws out what they mean, we don't have to know them we just need to know this is a non-linear model and everything to do with neurons is electrical. Hz (hertz) is spikes per second. 
</p>

<p>
You can access all these notebooks he's using for the demo <a href="https://github.com/jorchard/cs479.github.io/tree/master/demos">here</a> if interested.
</p>

<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=0FF1Y8triwE&amp;feature=youtu.be">Simpler Neuron Models</a> (28m) from <a href="https://jorchard.github.io/cs479.github.io/lectures.html">CS479</a>.</li>
</ul>

<p>
I don't have a physics background either but he's explaining the dynamics of a membrane well enough we can get the gist of what's going on. The difference between this first model and the previous Hodgkin-Huxley model is we are no longer modeling the spikes only the action potential for a spike to occur and a reset. Reduces complexity. @18:44 even simpler models. @22:20 ReLU and Softmax are introduced. We will later have to write our own Softmax.
</p>
</div>
</div>
<div id="outline-container-orgfeee97f" class="outline-2">
<h2 id="orgfeee97f">Day 2 Calculus</h2>
<div class="outline-text-2" id="text-orgfeee97f">
<p>
Let's look over the <a href="https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/pages/lecture-notes/">lecture notes</a> of MIT's 18.01. Most of this course we are skipping because you will always use software to calculate a derivative/integral we just need the absolute basics to begin learning matrix calculus. 
</p>
</div>
<div id="outline-container-org7725011" class="outline-3">
<h3 id="org7725011">Derivatives</h3>
<div class="outline-text-3" id="text-org7725011">
<p>
Reading <a href="https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/resources/lec1/">lecture 1</a>. If you want worked through examples then watch <a href="https://www.youtube.com/watch?v=Bjgu3i310pw">this</a> from Clemson's <a href="https://mthsc.clemson.edu/ug_course_pages/view_course_page/6">MATH 1060</a> but this will all be properly explained in higher dimensions when we do calculus in the realm of linear algebra. 
</p>

<p>
The answer to 'what is a tangent line exactly' is at that point P on the curve, super zoom until it's neighboring points appear to lay on a straight line. The slope of a tiny straight line between P and P + 0.001 is the tangent. Then you imagine doing this for the entire function creating many tiny little straight lines for every point in the function which is a linear approximation of that curve/function. 
</p>

<p>
The geometric definition pretend 'delta-x' \(\Delta x\) is a tiny displacement dx = 0.001 then imagine P + dx distance shrinking as dx approaches zero. Here is a <a href="https://www.desmos.com/calculator/3u37grlwxn">graph</a> (desmos.com) showing what is going on. Notice on that graph in order to go from (2, 4) to (2.001, 4.004) move to the right along the x-axis 0.001 then up 0.004 to meet the graph again. This is what the derivative tells us that if a function is preturbed by some tiny extra input f(x + dx) how sensitive is this function when we analyze the change in output or the rate of change. 
</p>

<p>
Example 1 of the MIT notes for f(x) = 1/x
</p>

<ul class="org-ul">
<li>Plug into derivative equation</li>
<li>Extract out 1/delta-x</li>
<li>Perform a/b - c/d is (ad - bc)/bd</li>
<li>Cancel the extracted delta-x leaving -1</li>
<li>Take limit to zero of delta-x</li>
<li>- 1/x<sup>2</sup></li>
</ul>

<p>
Why is it negative? Look at the <a href="https://www.desmos.com/calculator/vhr2djymiq">graph</a> if 0.001 is added to the x input then the y output has to drop down -0.5 to meet the graph again of f(x) = 1/x 
</p>

<p>
<i>Finding the tangent line</i> the equation for a line is y = mx + b and they have merely substituted in values but we will never use this feel free to skip it and the area computation after. 
</p>

<p>
Try inputs to f(x) = x<sup>2</sup> to understand Big-O 
</p>

<ul class="org-ul">
<li>(2 + 0.001) = 4.004001</li>
<li>(3 + 0.001) = 9.006001</li>
<li>(x + dx) = y + dy + 0.000001</li>
<li>(x + dx) = y + dy + O(dx)<sup>2</sup></li>
</ul>

<p>
If dx is approaching zero then (dx)<sup>2</sup> will reach zero before dx and can discarded. 
</p>

<p>
Those above inputs to f(x) = x<sup>2</sup> notice that dy = 2dx + (dx)<sup>2</sup> and if we ignore the (dx)<sup>2</sup> then dy/dx = 2 which is the derivative at that point. The derivative of the entire function is dy = (2x)dx or dy/dx = 2x. This is the instantaneous rate of change like if you were to take a picture of a speeding car and ask what is it's speed right now which is technically zero at an instantaneous snapshot but through the magic of limits we pretend the denominator of the derivative equation is so close to zero that no other positive quantity could wedge itself between the limit and zero. 
</p>

<p>
'dx' means 'a little bit of x' and dy means 'a little bit of y' it is infintesimal notation where an infintesimal is some extremely small positive quantity which is not zero. 
</p>
</div>
</div>
<div id="outline-container-org24d4257" class="outline-3">
<h3 id="org24d4257">Limits</h3>
<div class="outline-text-3" id="text-org24d4257">
<p>
Reading <a href="https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/resources/lec2/">lecture 2</a>. Watch <a href="https://www.youtube.com/watch?v=iQTwviLhQwU">left and right limits</a> and these <a href="https://www.youtube.com/watch?v=h7dfq5aD6E4">techniques</a> for computing limits such as multiplying by a conjugate which is how you eliminate square roots from limit calculations.  
</p>

<p>
A limit was already described perfectly by Isaac Newton as the ability to make the difference between a quantity and a fixed value less than any given positive number. He called these the ultimate ratio of 'vanishing quantities' where the ultimate ratio is not before they vanish or after but the ratio with which they vanish. 
</p>

<p>
These MIT notes tell us if f is differentiable at a point then f is continuous at that point which of course makes sense if we go back to the definition of a derivative being a linear approximation where around any super zoomed point is a tiny displacement point on a straight line (tangent).
</p>
</div>
<div id="outline-container-org64c0686" class="outline-4">
<h4 id="org64c0686">Trig functions</h4>
<div class="outline-text-4" id="text-org64c0686">
<p>
There's some trig limits here. If you forgot trig 3blue1brown has many YouTube lectures about sine and cosine or watch <a href="https://www.youtube.com/watch?v=XFpi6arBzgY">this</a>. Brown university also has a <a href="http://www.math.brown.edu/dkatz/trigbootcamp/">Trig Boot Camp</a>.  
</p>

<p>
We have just seen another linearization. Watch a few minutes of <a href="https://youtu.be/by0Gy1ZJ_hU?si=1TM8tv2c3TzE9xa_&amp;t=356">this</a> (Wildberger Rational Trig) starting @5:56 to see motion around a nonlinear curve being mapped to the linear motion on the two axis moving back and forth.
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org1cf291d" class="outline-2">
<h2 id="org1cf291d">Day 3 Integrals</h2>
<div class="outline-text-2" id="text-org1cf291d">
<p>
This is the only concept we will practice before matrix calculus as it will come up all the time in probability and elsewhere.
</p>

<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=HTa7jTU-72A">Indefinite integral</a> and <a href="https://www.youtube.com/watch?v=HTa7jTU-72A">Antiderivatives</a></li>
<li><a href="https://www.youtube.com/watch?v=lzCZLod2Z4I">Area approximations</a> (sets up Riemann sums)</li>
<li><a href="https://www.youtube.com/watch?v=pBXUUhb6YtU">Riemann sums</a> and the <a href="https://www.youtube.com/watch?v=o3sjROzOb3k">Definite Integral</a></li>
<li><a href="https://www.youtube.com/watch?v=FOgejRi7sNQ">Theorem of Calculus I</a> and <a href="https://www.youtube.com/watch?v=ntJmb8o9sOw">II</a></li>
</ul>

<p>
Those videos explain everything. Let's practice:
</p>

<ul class="org-ul">
<li><a href="https://personal.math.ubc.ca/~CLP/CLP2/clp_2_ic_problems.pdf">Integral problem book</a> from <a href="https://personal.math.ubc.ca/~CLP/">CLP textbooks</a></li>
<li><a href="https://integration.soc.srcf.net/UK_University_Integration_Bee_Techniques_Guide.pdf">Techniques</a> (Integration competition)</li>
</ul>

<p>
TODO
</p>

<hr />
<p>
<a href="./index.html">Home</a>
</p>
</div>
</div>
</div>
</body>
</html>
