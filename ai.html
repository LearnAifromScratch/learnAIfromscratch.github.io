<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2025-02-15 Sat 00:46 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>AI in 2025</title>
<meta name="author" content="jbh" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<style> body {background-color: #fafad2; max-width: 62.5rem; padding: 1rem; margin: auto;} </style>
</head>
<body>
<div id="content" class="content">
<h1 class="title">AI in 2025</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgf0a1bd3">Intro</a></li>
<li><a href="#orgb936410">Curriculum</a>
<ul>
<li><a href="#org3b13517">Linear Algebra options</a></li>
<li><a href="#orgc55f460">Linear Algebra resources I'll do here</a></li>
</ul>
</li>
</ul>
</div>
</div>
<style>details summary { color: green; }</style>

<div id="outline-container-orgf0a1bd3" class="outline-2">
<h2 id="orgf0a1bd3">Intro</h2>
<div class="outline-text-2" id="text-orgf0a1bd3">
<p>
We will learn the 'full stack' of modern deep learning systems by building our own library from scratch in your language of choice. Neural networks have different architectures just like there is different computer hardware architectures (x86, RISC, ARM). The most famous currently is transformer architecture for large language models like ChatGPT. We'll learn how to reverse engineer these models. When you are finished you can get paid for bounties to work on GeoHot's <a href="https://docs.google.com/spreadsheets/d/1WKHbT-7KOgjEawq5h5Ic1qUWzpfAzuD_J06N1JwOCGs/edit?gid=0#gid=0">TinyGrad</a>.
</p>

<p>
<a href="https://www.neelnanda.io/about">Neel Nanda</a> has helpfully produced a suggested <a href="https://www.neelnanda.io/mechanistic-interpretability/getting-started">curriculum</a> for reverse engineering transformers which we'll do. He even offers <a href="https://www.matsprogram.org/interpretability">mentorship</a> through the ML Alignment &amp; Theory Scholars program.
</p>
</div>

<div id="outline-container-org4735cff" class="outline-4">
<h4 id="org4735cff">Many iterations of this</h4>
<div class="outline-text-4" id="text-org4735cff">
<p>
I nuked this curriculum and stalled it dozens of times for the simple reason I went to work at a place that does this professionally and what we were doing then needed so many resources I thought it was a waste of time to teach it. Not anymore as the GPU bubble finally popped letting us plebs access to hardware. 
</p>

<p>
I learned from taking Waterloo's CS485 <a href="https://www.youtube.com/playlist?list=PLt6ES1TJ1gtsvVE_jD-nYaxB2yJzk4zNB">here</a> it is the best course for machine learning you will ever take and goes through the entire mathematical model assuming you have zero background by the Israeli author of <a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/">Understanding Machine Learning</a>. The theory of learning will not change. CMU still uses this book in their PhD track ML <a href="https://www.cs.cmu.edu/~nihars/teaching/10715-Fa23/index.html">course</a>.
</p>
</div>
</div>

<div id="outline-container-orgc2807ec" class="outline-4">
<h4 id="orgc2807ec">Limits of AI</h4>
<div class="outline-text-4" id="text-orgc2807ec">
<p>
Current popular AI is all <a href="https://en.wikipedia.org/wiki/Foundation_model">foundation models</a> like GPT-n, DALL-E etc. Have you wondered if we had infinite resources, infinite data, perfect training algorithms with no errors, can we use this type of model for everything aka 'General AI'? Someone with help from the Beijing Academy of AI used <a href="https://proceedings.mlr.press/v202/yuan23b.html">category theory</a> to model this scenario to see what is possible.
</p>
</div>
</div>
</div>

<div id="outline-container-orgb936410" class="outline-2">
<h2 id="orgb936410">Curriculum</h2>
<div class="outline-text-2" id="text-orgb936410">
<ul class="org-ul">
<li><a href="https://dlsyscourse.org/lectures/">10-414 Deep Learning Systems</a> (CMU)
<ul class="org-ul">
<li>Use any language you want</li>
</ul></li>
<li><a href="https://jorchard.github.io/cs479.github.io/index.html">CS 479 Neural Networks</a> (Waterloo)
<ul class="org-ul">
<li>Survey on neural networks from the perspective of theoretical neuroscience</li>
<li>Intuition building while hacking through 10-414</li>
</ul></li>
<li><a href="https://www.neelnanda.io/mechanistic-interpretability/getting-started">Mechanistic Interpretability</a> (Google DeepMind)
<ul class="org-ul">
<li>Neel's curriculum on reversing a trained neural network</li>
</ul></li>
<li><a href="https://ocw.mit.edu/courses/18-s096-matrix-calculus-for-machine-learning-and-beyond-january-iap-2023/">Matrix Calculus</a> (MIT)
<ul class="org-ul">
<li>All the calculus we need generalized to higher dimensions</li>
<li>IAP course or 'Independent Activities Period' where faculty can run a 4-week course</li>
</ul></li>
</ul>
</div>

<div id="outline-container-org3b13517" class="outline-3">
<h3 id="org3b13517">Linear Algebra options</h3>
<div class="outline-text-3" id="text-org3b13517">
<p>
There is no shortage of excellent linear algebra courses. These are only what I have reviewed there's tons more if you don't want to do them. 
</p>

<ul class="org-ul">
<li><a href="https://cs.brown.edu/courses/cs053/current/lectures.htm">Coding the Matrix</a> (Brown)
<ul class="org-ul">
<li>All the recorded lectures are open to anyone</li>
<li>Programmed w/Python and has a (nonfree) <a href="https://codingthematrix.com/">book</a>
<ul class="org-ul">
<li>I hate Python and did this book in OCaml and it worked out fine</li>
</ul></li>
</ul></li>
</ul>

<p>
Created by Philip Klein a name you will recognize if you take any algorithms course. He uses the complex field mostly to illustrate how linear transformations (mappings) work. The SVD is covered which we'll need. The book is at least cheap ($35) and worth buying or you can use Anna's Archive or whatever the latest Library Genesis domain is to get a pdf but it will likely be missing a lot of graphical content. This is a very good course for anyone interested in game graphics or taking an algorithms design course and wants to manipulate graphs using linear algebra. If you hate everything else here then do this you'll be fine.  
</p>

<ul class="org-ul">
<li><a href="https://github.com/mitmath/1806/blob/spring20/summaries.md">Modernized 18.06</a> (MIT)
<ul class="org-ul">
<li>Alan Edelman's course that teaches intuition</li>
<li>No pivots, no echelon forms, no free variables, no hand computation</li>
<li>Treats the SVD as it's own independent thing not some Eigenwhatever</li>
<li>Almost entirely programmed</li>
<li>Lectures got locked up by MIT logins but were once open</li>
<li>All other materials like solutions to homework/recitations are open</li>
</ul></li>
</ul>

<p>
Strang had a stranglehold on MIT's linear algebra curriculum for decades and Edelman came along and shredded it. Now that Strang is retired maybe we'll see the new 18.06 lectures soon. I simply can't follow any book or lecture by Strang it's just disjointed rambling to me.  
</p>

<ul class="org-ul">
<li><a href="https://linear.axler.net/">Linear Algebra Done Right</a> - Sheldon Axler
<ul class="org-ul">
<li>New completely free 4th version</li>
<li>He upgraded his cat too that was always in the about author pic</li>
<li>Abstract treatment which we need but is considered a second course</li>
<li>Contains some calculus</li>
<li>Seems designed to prepare students for functional analysis (and ML)</li>
</ul></li>
</ul>


<p>
Everyone will tell you to read this and we probably have to do this because we need the abstract higher dimensional theory. He banishes determinants for a good reason all laid out in a paper you can find on arxiv. It's great and the proofs are not difficult the way he writes them are clear and intuitive. He doesn't explain what is going on though at all though there is some short YouTube geometry examples he made for the book but then again this is a 'second course' so he doesn't need to. Every mathematician will tell you that is how you learn meaning figure it out yourself and piece it all together seeing that function composition is non-associative just like matrix multiplication ergo a matrix is some kind of encoding of a function (mapping).  
</p>

<ul class="org-ul">
<li><a href="https://terrytao.wordpress.com/wp-content/uploads/2016/12/linear-algebra-notes.pdf">Terence Tao's Notes</a> for <a href="https://www.math.ucla.edu/~tao/resource/general/115a.3.02f/">Math 115A</a>
<ul class="org-ul">
<li>The best from scratch linear algebra notes anywhere</li>
</ul></li>
</ul>

<p>
These notes were his lectures to accompany the book by Friedberg, Insel and Spence. CMU uses the David Lay book. A lot of other universities like Stanford use <a href="https://web.stanford.edu/~boyd/vmls/">VMLS</a> (free). VMLS also has a Julia language companion. 
</p>


<ul class="org-ul">
<li><a href="http://www.math.clemson.edu/~macaule/classes/s21_math8530/">Math 8530</a> Graduate Linear Algebra (Clemson)
<ul class="org-ul">
<li>Theory of vector spaces not abstract modules</li>
<li>Loosely follows Peter Lax's book chapters 1-9 and Halmos' book on finite-dimensional vector spaces
<ul class="org-ul">
<li>Edelman took this in highschool</li>
</ul></li>
</ul></li>
</ul>

<p>
We could take this. It's highly abstract but not impossibly abstract like replacing scalars in a vector space with entire rings like module theory. Peter Lax when he wrote this book was the foremost expert on computational partial differential equations. PDEs are now being 'solved' by neural networks so there is a quiet scientific field going right now where physics informed machine learning injects physics knowledge to neural networks to supercharge semi-supervised learning. This is where all the early revolutionary AI will come from when we can accelerate training.   
</p>

<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=ndWqKOJ9DgQ">Geometric Linear Algebra</a> NJ Wildberger
<ul class="org-ul">
<li>Tells you what is actually going on</li>
<li>Functionals are covered which we'll need for Matrix calculus</li>
</ul></li>
</ul>

<p>
We will do this but with one of the above resources. It somehow manages to be extremely concrete while being extemely abstract which is what we want.    
</p>

<ul class="org-ul">
<li><a href="http://matrixeditions.com/5thUnifiedApproach.html">Hubbard &amp; Hubbard</a> book on unified algebra</li>
</ul>

<p>
Have you ever wanted to redo everything from scratch using linear algebra? There's a book for that. It's like redoing highschool to undergrad but with linear algebra. It's 800+ pages. My only beef with this book is it pretends the Riemann integral is anything except a waste of time. Don't get me wrong Riemann sums can be useful in some situations but the integral we are taught in highschool and undergrad is a joke. Riemann scribbled his integral on a napkin as a temporary fix over 100+ years ago and you should not waste you time learning it. No ML paper will use it anyway it will all be Lebesgue measure or spaces (hopefully). If they don't it will be a convoluted mess you have to interpret with bastardized notation as they try and get around all the Riemann shortcomings. Imagine you took multiple semesters of calculus and analysis in undergrad only to be told on your first day of grad instruction all that shit is tossed out and there is no refunds.
</p>
</div>
</div>

<div id="outline-container-orgc55f460" class="outline-3">
<h3 id="orgc55f460">Linear Algebra resources I'll do here</h3>
<div class="outline-text-3" id="text-orgc55f460">
<ul class="org-ul">
<li>Axler's latest <a href="https://linear.axler.net/">book</a>
<ul class="org-ul">
<li>We have no choice but to take this for higher-dimensional ML</li>
</ul></li>
<li>Wldberger's intuitive <a href="https://www.youtube.com/watch?v=yAb12PWrhV0">lectures</a> to figure out Axler</li>
<li>Poh-Shen Loh's <a href="https://www.youtube.com/watch?v=5__JFBncHAA">Matrices</a> Teacher Professional Development</li>
<li>Poh-Shen Loh's <a href="https://www.youtube.com/watch?v=KfiCOUIvjE8">Determinant</a> Teacher Professional Development</li>
<li>Poh-Shen Loh's <a href="https://www.youtube.com/watch?v=uWU0j98jBRo">Putnam Seminar</a> on Linear Algebra</li>
</ul>

<p>
Watching Poh-Shen Loh explain why Matrix multiplication works is amazing. He's definitely my favorite mathematican because he was just a regular guy who failed linear algebra the first time he took it then went on to become to coach of the US olympiad team after thinking they were going to toss him his second year. He survived by going for broke and martingaling on cruise control (making heavy bets to get yourself out of a debt hole) where he decided to prove everything in the olympiad seminar himself in the most creative way possible a huge gamble that paid off.
</p>

<p>
We can do this while we do CMU's hacker course on rewriting PyTorch in our language of choice because I personally hate Python like you wouldn't believe because of it's clown amateur scope and other reasons you don't care about. I would rather abandon my entire career than write a single line of Python. I would rather go back to labor then design some 1990s "classes" so I never try and inflict that on anons. You would think this would sink me as a career professional who works with AI everyday but I never compromised and targeted companies where it didn't matter. My fanaticism paid off I went full martingaling on codebux just like Poh-Shen Loh gambled it all. This is why I try and push language independent resources whenever I find them. Yes I'm fully aware it's all the same thing but imperative programming is work to me like doing manual labor. Functional programming in Lisp or Standard ML is not labor to me. I still do manual labor! Like everyday I'm still a labor pleb and never gave up my working class gig (union pension) plus I'm doing 8 hours of intensive compsci but it's not work to me so I enjoy it. Anyway that is my rant and now you know my bias so take it into account while you do this workshop. Assembly however is fun. That's why I take it <a href="./taocp.html">here</a>. Algol60 reinvented garbage is not fun so it's not here. Anyway that is my rant, now you know my position, and who cares let's learn some linear algebra. 
</p>

<p>
TODO
</p>

<hr />
<p>
<a href="./index.html">Home</a>
</p>
</div>
</div>
</div>
</div>
</body>
</html>
