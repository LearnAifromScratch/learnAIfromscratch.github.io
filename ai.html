<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2025-07-23 Wed 06:56 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>AI in 2025</title>
<meta name="author" content="jbh" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<style> body {background-color: #fafad2; max-width: 62.5rem; padding: 1rem; margin: auto;} </style>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">AI in 2025</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org8fa8ff2">Intro</a></li>
<li><a href="#orgb7b11a8">Curriculum</a>
<ul>
<li><a href="#org9796135">Linear Algebra</a></li>
<li><a href="#org97e09f3">Basic calculus</a></li>
</ul>
</li>
<li><a href="#org4f894d9">Day 1 Neuroscience Models</a></li>
<li><a href="#orga23f84e">Day 2 Calculus</a>
<ul>
<li><a href="#orgc7b7f26">Derivatives</a></li>
<li><a href="#org7205371">Limits</a></li>
</ul>
</li>
<li><a href="#org02d768a">Day 3 Integrals</a></li>
</ul>
</div>
</div>


<div id="outline-container-org8fa8ff2" class="outline-2">
<h2 id="org8fa8ff2">Intro</h2>
<div class="outline-text-2" id="text-org8fa8ff2">
<p>
We will learn the 'full stack' of modern deep learning systems by building our own library from scratch in your language of choice. Neural networks have different architectures just like there is different computer hardware architectures (x86, RISC, ARM). The most popular currently is transformer architecture for large language models like ChatGPT. We'll learn how to reverse engineer these models. 
</p>

<p>
<a href="https://www.neelnanda.io/about">Neel Nanda</a> has helpfully produced a suggested <a href="https://www.neelnanda.io/mechanistic-interpretability/getting-started">curriculum</a> for reverse engineering transformers. He even offers <a href="https://www.matsprogram.org/interpretability">mentorship</a> through the ML Alignment &amp; Theory Scholars program or you can claim bounties for GeoHot's <a href="https://docs.google.com/spreadsheets/d/1WKHbT-7KOgjEawq5h5Ic1qUWzpfAzuD_J06N1JwOCGs/edit?gid=0#gid=0">TinyGrad</a> as a freelancer.
</p>
</div>

<div id="outline-container-org5ac0304" class="outline-4">
<h4 id="org5ac0304">Theory</h4>
<div class="outline-text-4" id="text-org5ac0304">
<p>
Waterloo's CS485 <a href="https://www.youtube.com/playlist?list=PLt6ES1TJ1gtsvVE_jD-nYaxB2yJzk4zNB">here</a> is the best course for machine learning theory you will ever take and goes through the entire mathematical model assuming you have zero background by the Israeli author of <a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/">Understanding Machine Learning</a>. The theory of learning will not change so this is timeless. CMU still uses this book in their PhD track ML <a href="https://www.cs.cmu.edu/~nihars/teaching/10715-Fa23/index.html">course</a>. I'll go through it in Part II <a href="./math.html">here</a>.
</p>
</div>
</div>

<div id="outline-container-org36b7c73" class="outline-4">
<h4 id="org36b7c73">Limits of AI</h4>
<div class="outline-text-4" id="text-org36b7c73">
<p>
Current popular AI is all <a href="https://en.wikipedia.org/wiki/Foundation_model">foundation models</a> like Grok-n, GPT-n, DALL-E etc. Have you wondered if we had infinite resources, infinite data, perfect training algorithms with no errors, can we use this type of model for everything aka 'General AI'? Someone with help from the Beijing Academy of AI used <a href="https://proceedings.mlr.press/v202/yuan23b.html">category theory</a> to model this scenario to see what is possible.
</p>
</div>
</div>
</div>

<div id="outline-container-orgb7b11a8" class="outline-2">
<h2 id="orgb7b11a8">Curriculum</h2>
<div class="outline-text-2" id="text-orgb7b11a8">
<ul class="org-ul">
<li><a href="https://dlsyscourse.org/lectures/">10-414 Deep Learning Systems</a> (CMU)
<ul class="org-ul">
<li>Use any language you want</li>
</ul></li>
<li><a href="https://jorchard.github.io/cs479.github.io/index.html">CS 479 Neural Networks</a> (Waterloo)
<ul class="org-ul">
<li>Survey on neural networks from the perspective of theoretical neuroscience</li>
<li>Intuition building while hacking through 10-414</li>
</ul></li>
<li><a href="https://www.neelnanda.io/mechanistic-interpretability/getting-started">Mechanistic Interpretability</a> (Google DeepMind)
<ul class="org-ul">
<li>Neel's curriculum on reversing a trained neural network</li>
</ul></li>
<li><a href="https://ocw.mit.edu/courses/18-s096-matrix-calculus-for-machine-learning-and-beyond-january-iap-2023/">Matrix Calculus</a> (MIT)
<ul class="org-ul">
<li>All the calculus we need generalized to higher dimensions</li>
<li>IAP course or 'Independent Activities Period' where faculty can run a 4-week course</li>
</ul></li>
</ul>
</div>

<div id="outline-container-org9796135" class="outline-3">
<h3 id="org9796135">Linear Algebra</h3>
<div class="outline-text-3" id="text-org9796135">
<p>
There is no shortage of excellent linear algebra courses. These are the one's I will do here:
</p>

<ul class="org-ul">
<li><a href="https://cs.brown.edu/courses/cs053/current/lectures.htm">Coding the Matrix</a> (Brown)
<ul class="org-ul">
<li>All the recorded lectures are open to anyone</li>
<li>Programmed w/Python and has a (nonfree) <a href="https://codingthematrix.com/">book</a></li>
</ul></li>
</ul>

<p>
Created by Philip Klein a name you will recognize if you take any algorithms course. Probability and the SVD is covered which we'll need. The book is at least cheap ($35) and worth buying or you can use <a href="https://annas-archive.org/slow_download/13d7370d8f4ea2d2c3885be0a21bc01e/0/0">Anna's Archive</a> or whatever the <a href="https://open-slum.org/">latest</a> Library Genesis domain is to get a pdf but it will likely be missing a lot of graphical content. This is a very good course for anyone interested in game graphics or taking an algorithms design course and wants to manipulate graphs using linear algebra. If you hate everything else here then do this you'll be fine.  
</p>

<ul class="org-ul">
<li><a href="https://linear.axler.net/">Linear Algebra Done Right</a> - Sheldon Axler
<ul class="org-ul">
<li>New completely free 4th version</li>
<li>He upgraded his cat too that was always in the about author pic</li>
<li>Abstract treatment which we need but is considered a second course</li>
<li>Contains some calculus</li>
<li>Seems designed to prepare students for functional analysis (and thus ML)</li>
</ul></li>
</ul>

<p>
Neel Nanda says we should we do this and it can be completed in parallel with the Klein book.
</p>
</div>
</div>

<div id="outline-container-org97e09f3" class="outline-3">
<h3 id="org97e09f3">Basic calculus</h3>
<div class="outline-text-3" id="text-org97e09f3">
<p>
We can audit some of the slides from MIT <a href="https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/">OpenCourseWare</a> then the Matrix Calculus course will teach us the behavior of derivatives.
</p>
</div>
</div>
</div>


<div id="outline-container-org4f894d9" class="outline-2">
<h2 id="org4f894d9">Day 1 Neuroscience Models</h2>
<div class="outline-text-2" id="text-org4f894d9">
<p>
Reminder this is 'learn AI from scratch' thus here is neural models from scratch. 
</p>

<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=uBHW5Rs7qsA&amp;feature=youtu.be">What is a Neuron</a> (26m) from <a href="https://jorchard.github.io/cs479.github.io/index.html">CS479</a>.
<ul class="org-ul">
<li>Hodgkin-Huxley Neuron Model</li>
</ul></li>
</ul>

<p>
@6:39 his example of an action potential using a water bucket being filled and dumped is exactly the same intuition of a potential function in a data structures analysis course used for amortized analysis. A dynamic array once filled up to a certain percentage needs to perform a costly action of doubling itself to enable more free space. You take the infrequent cost of doubling and average it over many read/writes to come up with an individual cost of all array operations. There's some differential equations here but he draws out what they mean, we don't have to know them we just need to know this is a non-linear model and everything to do with neurons is electrical. Hz (hertz) is spikes per second. 
</p>

<p>
You can access all these notebooks he's using for the demo <a href="https://github.com/jorchard/cs479.github.io/tree/master/demos">here</a> if interested.
</p>

<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=0FF1Y8triwE&amp;feature=youtu.be">Simpler Neuron Models</a> (28m) from <a href="https://jorchard.github.io/cs479.github.io/lectures.html">CS479</a>.</li>
</ul>

<p>
I don't have a physics background either but he's explaining the dynamics of a membrane well enough we can get the gist of what's going on. The difference between this first model and the previous Hodgkin-Huxley model is we are no longer modeling the spikes only the action potential for a spike to occur and a reset. Reduces complexity. @18:44 even simpler models. @22:20 ReLU and Softmax are introduced. We will later have to write our own Softmax.
</p>
</div>
</div>

<div id="outline-container-orga23f84e" class="outline-2">
<h2 id="orga23f84e">Day 2 Calculus</h2>
<div class="outline-text-2" id="text-orga23f84e">
<p>
Let's look over the <a href="https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/pages/lecture-notes/">lecture notes</a> of MIT's 18.01. Most of this course we are skipping because you will always use software to calculate a derivative/integral we just need the absolute basics to begin learning matrix calculus. 
</p>
</div>

<div id="outline-container-orgc7b7f26" class="outline-3">
<h3 id="orgc7b7f26">Derivatives</h3>
<div class="outline-text-3" id="text-orgc7b7f26">
<p>
Reading <a href="https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/resources/lec1/">lecture 1</a>. If you want worked through examples then watch <a href="https://www.youtube.com/watch?v=Bjgu3i310pw">this</a> from Clemson's <a href="https://mthsc.clemson.edu/ug_course_pages/view_course_page/6">MATH 1060</a> but this will all be properly explained in higher dimensions when we do calculus in the realm of linear algebra. 
</p>

<p>
The answer to 'what is a tangent line exactly' is at that point P on the curve, super zoom until it's neighboring points appear to lay on a straight line. The slope of a tiny straight line between P and P + 0.001 is the tangent. Then you imagine doing this for the entire function creating many tiny little straight lines for every point in the function which is a linear approximation of that curve/function.
</p>

<p>
The geometric definition pretend 'delta-x' \(\Delta x\) is a tiny displacement dx = 0.001 then imagine P + dx distance shrinking as dx approaches zero. Here is a <a href="https://www.desmos.com/calculator/3u37grlwxn">graph</a> (desmos.com) showing what is going on. Notice on that graph in order to go from (2, 4) to (2.001, 4.004) move to the right along the x-axis 0.001 then up 0.004 to meet the graph again. This is what the derivative tells us that if a function is preturbed by some tiny extra input f(x + dx) how sensitive is this function when we analyze the change in output. 
</p>

<p>
Example 1 of the MIT notes for f(x) = 1/x
</p>

<ul class="org-ul">
<li>Plug into derivative equation</li>
<li>Extract out 1/delta-x</li>
<li>Perform a/b - c/d is (ad - bc)/bd</li>
<li>Cancel the extracted delta-x leaving -1</li>
<li>Take limit to zero of delta-x</li>
<li>- 1/x<sup>2</sup></li>
</ul>

<p>
Why is it negative? Look at the <a href="https://www.desmos.com/calculator/vhr2djymiq">graph</a> if 0.001 is added to the x input then the y output has to drop down -0.5 to meet the graph again of f(x) = 1/x 
</p>

<p>
<i>Finding the tangent line</i> the equation for a line is y = mx + b and they have merely substituted in values but we will never use this feel free to skip it and the area computation after. 
</p>

<p>
Try inputs to f(x) = x<sup>2</sup> to understand Big-O 
</p>

<ul class="org-ul">
<li>(2 + 0.001) = 4.004001</li>
<li>(3 + 0.001) = 9.006001</li>
<li>(x + dx) = y + dy + 0.000001</li>
<li>(x + dx) = y + dy + O(dx)<sup>2</sup></li>
</ul>

<p>
If dx is approaching zero then (dx)<sup>2</sup> will reach zero before dx and can discarded. 
</p>

<p>
Those above inputs to f(x) = x<sup>2</sup> notice that dy = 2dx + (dx)<sup>2</sup> and if we ignore the (dx)<sup>2</sup> then dy/dx = 2 which is the derivative at that point. The derivative of the entire function is dy = (2x)dx or dy/dx = 2x.
</p>

<p>
'dx' means 'a little bit of x' and dy means 'a little bit of y' it is infintesimal notation where an infintesimal is some extremely small positive quantity which is not zero. 
</p>
</div>
</div>

<div id="outline-container-org7205371" class="outline-3">
<h3 id="org7205371">Limits</h3>
<div class="outline-text-3" id="text-org7205371">
<p>
Reading <a href="https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/resources/lec2/">lecture 2</a>. Watch <a href="https://www.youtube.com/watch?v=iQTwviLhQwU">left and right limits</a> and these <a href="https://www.youtube.com/watch?v=h7dfq5aD6E4">techniques</a> for computing limits such as multiplying by a conjugate which is how you eliminate square roots from limit calculations.  
</p>

<p>
A limit was already described perfectly by Isaac Newton as the ability to make the difference between a quantity and a fixed value less than any given positive number. He called these the ultimate ratio of 'vanishing quantities' where the ultimate ratio is not before they vanish or after but the ratio with which they vanish. 
</p>

<p>
These MIT notes tell us if f is differentiable at a point then f is continuous at that point which of course makes sense if we go back to the definition of a derivative being a linear approximation where around any super zoomed point is a tiny displacement point on a straight line (tangent).
</p>
</div>

<div id="outline-container-org2ce682a" class="outline-4">
<h4 id="org2ce682a">Trig functions</h4>
<div class="outline-text-4" id="text-org2ce682a">
<p>
There's some trig limits here. If you forgot trig 3blue1brown has many YouTube lectures about sine and cosine or watch <a href="https://www.youtube.com/watch?v=XFpi6arBzgY">this</a>. Brown university also has a <a href="http://www.math.brown.edu/dkatz/trigbootcamp/">Trig Boot Camp</a>.  
</p>

<p>
We have just seen another linearization. Watch a few minutes of <a href="https://youtu.be/by0Gy1ZJ_hU?si=1TM8tv2c3TzE9xa_&amp;t=356">this</a> (Wildberger Rational Trig) starting @5:56 to see motion around a nonlinear curve being mapped to the linear motion on the two axis moving back and forth.
</p>
</div>
</div>
</div>
</div>


<div id="outline-container-org02d768a" class="outline-2">
<h2 id="org02d768a">Day 3 Integrals</h2>
<div class="outline-text-2" id="text-org02d768a">
<p>
This is the only concept we will practice before matrix calculus as it will come up all the time in probability and elsewhere.
</p>

<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=HTa7jTU-72A">Indefinite integral</a> and <a href="https://www.youtube.com/watch?v=HTa7jTU-72A">Antiderivatives</a></li>
<li><a href="https://www.youtube.com/watch?v=lzCZLod2Z4I">Area approximations</a> (sets up Riemann sums)</li>
<li><a href="https://www.youtube.com/watch?v=pBXUUhb6YtU">Riemann sums</a> and the <a href="https://www.youtube.com/watch?v=o3sjROzOb3k">Definite Integral</a></li>
<li><a href="https://www.youtube.com/watch?v=FOgejRi7sNQ">Theorem of Calculus I</a> and <a href="https://www.youtube.com/watch?v=ntJmb8o9sOw">II</a></li>
</ul>

<p>
Those videos explain everything. Let's practice:
</p>

<ul class="org-ul">
<li><a href="https://personal.math.ubc.ca/~CLP/CLP2/clp_2_ic_problems.pdf">Integral problem book</a> from <a href="https://personal.math.ubc.ca/~CLP/">CLP textbooks</a></li>
<li><a href="https://integration.soc.srcf.net/UK_University_Integration_Bee_Techniques_Guide.pdf">Techniques</a> (Integration competition)</li>
</ul>

<p>
TODO
</p>

<hr />
<p>
<a href="./index.html">Home</a>
</p>
</div>
</div>
</div>
</body>
</html>
