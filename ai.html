<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2025-02-13 Thu 23:37 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>AI in 2025</title>
<meta name="author" content="jbh" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<style> body {background-color: #fafad2; max-width: 62.5rem; padding: 1rem; margin: auto;} </style>
</head>
<body>
<div id="content" class="content">
<h1 class="title">AI in 2025</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgca94050">Intro</a></li>
<li><a href="#org602d063">Curriculum</a>
<ul>
<li><a href="#org4c31ccf">PreReq: Linear Algebra</a></li>
</ul>
</li>
</ul>
</div>
</div>
<style>details summary { color: green; }</style>

<div id="outline-container-orgca94050" class="outline-2">
<h2 id="orgca94050">Intro</h2>
<div class="outline-text-2" id="text-orgca94050">
<p>
We will learn the 'full stack' of modern deep learning systems by building our own library from scratch in your language of choice. Neural networks have different architectures just like there is different computer hardware architectures (x86, RISC, ARM). The most famous currently is transformer architecture for large language models like ChatGPT. We'll learn how to reverse engineer these models. When you are finished you can get paid for bounties to work on GeoHot's <a href="https://docs.google.com/spreadsheets/d/1WKHbT-7KOgjEawq5h5Ic1qUWzpfAzuD_J06N1JwOCGs/edit?gid=0#gid=0">TinyGrad</a>.
</p>

<p>
<a href="https://www.neelnanda.io/about">Neel Nanda</a> has helpfully produced a suggested <a href="https://www.neelnanda.io/mechanistic-interpretability/getting-started">curriculum</a> for reverse engineering transformers which we'll do. He even offers <a href="https://www.matsprogram.org/interpretability">mentorship</a> through the ML Alignment &amp; Theory Scholars program.
</p>
</div>

<div id="outline-container-org2070b39" class="outline-4">
<h4 id="org2070b39">Many iterations of this</h4>
<div class="outline-text-4" id="text-org2070b39">
<p>
I nuked this curriculum and stalled it dozens of times for the simple reason I went to work at a place that does this professionally and what we were doing then needed so many resources I thought it was a waste of time to teach it. Not anymore as the GPU bubble finally popped letting us plebs access to hardware. 
</p>

<p>
I learned from taking Waterloo's CS485 <a href="https://www.youtube.com/playlist?list=PLt6ES1TJ1gtsvVE_jD-nYaxB2yJzk4zNB">here</a> it is the best course for machine learning you will ever take and goes through the entire mathematical model assuming you have zero background by the Israeli author of <a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/">Understanding Machine Learning</a>. The theory of learning will not change. CMU still uses this book in their PhD track ML <a href="https://www.cs.cmu.edu/~nihars/teaching/10715-Fa23/index.html">course</a>.
</p>
</div>
</div>

<div id="outline-container-orga0cbae7" class="outline-4">
<h4 id="orga0cbae7">Limits of AI</h4>
<div class="outline-text-4" id="text-orga0cbae7">
<p>
Current popular AI is all <a href="https://en.wikipedia.org/wiki/Foundation_model">foundation models</a> like GPT-n, DALL-E etc. Have you wondered if we had infinite resources, infinite data, perfect training algorithms with no errors, can we use this type of model for everything aka 'General AI'? Someone with help from the Beijing Academy of AI used <a href="https://proceedings.mlr.press/v202/yuan23b.html">category theory</a> to model this scenario to see what is possible.
</p>
</div>
</div>
</div>

<div id="outline-container-org602d063" class="outline-2">
<h2 id="org602d063">Curriculum</h2>
<div class="outline-text-2" id="text-org602d063">
<ul class="org-ul">
<li><a href="https://dlsyscourse.org/lectures/">10-414 Deep Learning Systems</a> (CMU)
<ul class="org-ul">
<li>Use any language you want</li>
</ul></li>
<li><a href="https://jorchard.github.io/cs479.github.io/index.html">CS 479 Neural Networks</a> (Waterloo)
<ul class="org-ul">
<li>Survey on neural networks from the perspective of theoretical neuroscience</li>
<li>Intuition building while hacking through 10-414</li>
</ul></li>
<li><a href="https://www.neelnanda.io/mechanistic-interpretability/getting-started">Mechanistic Interpretability</a> (Google DeepMind)
<ul class="org-ul">
<li>Neel's curriculum on reversing a trained neural network</li>
</ul></li>
<li><a href="https://ocw.mit.edu/courses/18-s096-matrix-calculus-for-machine-learning-and-beyond-january-iap-2023/">Matrix Calculus</a> (MIT)
<ul class="org-ul">
<li>All the calculus we need generalized to higher dimensions</li>
<li>IAP course or 'Independent Activities Period' where faculty can run a 4-week course</li>
</ul></li>
</ul>
</div>

<div id="outline-container-org4c31ccf" class="outline-3">
<h3 id="org4c31ccf">PreReq: Linear Algebra</h3>
<div class="outline-text-3" id="text-org4c31ccf">
<p>
There is no shortage of excellent linear algebra courses. 
</p>

<ul class="org-ul">
<li><a href="https://cs.brown.edu/courses/cs053/current/lectures.htm">Coding the Matrix</a> (Brown)
<ul class="org-ul">
<li>All the recorded lectures are open to anyone</li>
<li>Programmed w/Python and has a (nonfree) <a href="https://codingthematrix.com/">book</a></li>
<li>Includes auto-grading!</li>
</ul></li>
</ul>

<p>
Created by Philip Klein a name you will recognize if you take any algorithms course. He uses the complex field mostly to illustrate how linear transformations (mappings) work. The SVD is covered which we'll need. The book is at least cheap ($35) and worth buying or you can use Anna's Archive or whatever the latest Library Genesis domain is to get a pdf but it will likely be missing a lot of graphical content. 
</p>

<ul class="org-ul">
<li><a href="https://github.com/mitmath/1806/blob/spring20/summaries.md">Modernized 18.06</a> (MIT)
<ul class="org-ul">
<li>Alan Edelman's course that teaches intuition</li>
<li>No pivots, no echelon forms, no free variables, no hand computation</li>
<li>Treats the SVD as it's own independent thing not some Eigenwhatever</li>
<li>Almost entirely programmed</li>
<li>Lectures got locked up by MIT logins but were once open</li>
<li>All other materials like solutions to homework is open</li>
</ul></li>
</ul>

<p>
Strang had a stranglehold on MIT's linear algebra curriculum for decades and Edelman came along and shredded it. Now that Strang is retired maybe we'll see the new 18.06 lectures soon. I simply can't follow any book or lecture by Strang it's just disjointed rambling to me.
</p>

<ul class="org-ul">
<li><a href="https://linear.axler.net/">Linear Algebra Done Right</a> - Sheldon Axler
<ul class="org-ul">
<li>New completely free 4th version</li>
<li>He upgraded his cat too that was always in the about author pic</li>
<li>Abstract treatment which we need but is considered a 'second course'</li>
<li>Contains some calculus</li>
<li>Seems designed to prepare students for functional analysis</li>
</ul></li>
</ul>

<p>
Everyone will tell you to read this. It's great and the proofs are not difficult the way he writes them are clear and intuitive. He doesn't explain what is going on though at all. Not even a tiny bit. Every mathematician will tell you that how you learn is to figure it out yourself and piece it all together seeing that function composition is non-associative just like matrix multiplication ergo a matrix is some kind of encoding of a function (mapping). This seems to be designed as a university text so if you're teaching yourself without TA's and lectures you may miss a lot of things he considers obvious because he's a leading mathematician and just assumes you already know because everyone reading this will have already done many months of vector calculus.
</p>

<ul class="org-ul">
<li><a href="https://terrytao.wordpress.com/wp-content/uploads/2016/12/linear-algebra-notes.pdf">Terence Tao's Notes</a> for <a href="https://www.math.ucla.edu/~tao/resource/general/115a.3.02f/">Math 115A</a>
<ul class="org-ul">
<li>The best from scratch linear algebra notes anywhere</li>
</ul></li>
</ul>


<p>
These notes were his lectures to accompany the book by Friedberg, Insel and Spence. CMU uses the David Lay book. A lot of other universities like Stanford use <a href="https://web.stanford.edu/~boyd/vmls/">VMLS</a> (free). VMLS also has a Julia language companion.
</p>


<ul class="org-ul">
<li><a href="http://www.math.clemson.edu/~macaule/classes/s21_math8530/">Math 8530</a> Graduate Linear Algebra (Clemson)
<ul class="org-ul">
<li>Theory of vector spaces not abstract modules</li>
<li>Loosely follows Peter Lax's book chapters 1-9 and Halmos' book on finite-dimensional vector spaces
<ul class="org-ul">
<li>Edelman took this in highschool</li>
</ul></li>
</ul></li>
</ul>

<p>
We could take this. It's highly abstract but not impossibly abstract like replacing scalars in a vector space with entire rings like module theory. Peter Lax when he wrote this book was the foremost expert on computational partial differential equations. PDEs are now being 'solved' by neural networks so there is a largely quiet scientific field going right now where physics informed machine learning injects physics knowledge of a complex system to neural networks to supercharge semi-supervised learning. This is where all the early revolutionary AI will come from when we can accelerate training.   
</p>

<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=ndWqKOJ9DgQ">Geometric Linear Algebra</a> NJ Wildberger
<ul class="org-ul">
<li>Tells you what is actually going on</li>
<li>Functionals are covered which we'll need for Matrix calculus</li>
</ul></li>
</ul>

<p>
Ever since I watched most of these I can now visualize mappings and projections in my head easily so I'm going to go through most of the playlist here as it's the only material I know of which builds it all from scratch. We only need an intuitive understanding of linear algebra in order to take MIT's Matrix Calculus.
</p>

<p>
TODO 
</p>



<hr />
<p>
<a href="./index.html">Home</a>
</p>
</div>
</div>
</div>
</div>
</body>
</html>
