<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2025-11-11 Tue 17:47 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>AI in 2025</title>
<meta name="author" content="jbh" />
<meta name="generator" content="Org Mode" />
<style type="text/css">
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<style> body {background-color: #fafad2; max-width: 62.5rem; padding: 1rem; margin: auto;} </style>
</head>
<body>
<div id="content" class="content">
<h1 class="title">AI in 2025</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org2c32552">Intro</a></li>
<li><a href="#org62d1402">Curriculum</a>
<ul>
<li><a href="#org01b0b52">What we want to achieve</a></li>
<li><a href="#org326bece">How to get there</a>
<ul>
<li><a href="#org81cb456">Intro to machine learning</a></li>
<li><a href="#org114a723">Neural Networks</a></li>
</ul>
</li>
<li><a href="#orge0cf68a">Math we need</a></li>
</ul>
</li>
<li><a href="#org1707a66">Day 1 Scalar Calculus</a>
<ul>
<li><a href="#org75f016d">Day 1 task</a></li>
</ul>
</li>
<li><a href="#org0f47043">Day 2 Neuroscience Models</a></li>
</ul>
</div>
</div>
<div id="outline-container-org2c32552" class="outline-2">
<h2 id="org2c32552">Intro</h2>
<div class="outline-text-2" id="text-org2c32552">
<p>
We will learn the 'full stack' of modern deep learning systems by building our own library from scratch. Neural networks have different architectures just like there is different computer hardware architectures (x86, RISC, ARM). The most popular currently is transformer architecture for large language models (Grok-n, GPT-n).   
</p>

<p>
<a href="https://www.neelnanda.io/about">Neel Nanda</a> has helpfully produced a suggested <a href="https://www.neelnanda.io/mechanistic-interpretability/getting-started">curriculum</a> for reverse engineering transformers. He even offers <a href="https://www.matsprogram.org/interpretability">mentorship</a> through the ML Alignment &amp; Theory Scholars program or you can claim bounties for GeoHot's <a href="https://docs.google.com/spreadsheets/d/1WKHbT-7KOgjEawq5h5Ic1qUWzpfAzuD_J06N1JwOCGs/edit?gid=0#gid=0">TinyGrad</a> as a freelancer.
</p>

<p>
There is no prereqs except ability to program I made everything into self-directed research so when math we don't know comes up then we look it up and learn it <i>in the context</i> of what we're currently doing. Otherwise you will spend forever drilling scalar and vector calculus which you may want to do after this but then you'll have motivation to finish it. 
</p>


<p>
<b>Limits of AI</b>
</p>

<p>
Current popular AI is all <a href="https://en.wikipedia.org/wiki/Foundation_model">foundation models</a> like Grok-n, GPT-n, DALL-E etc. Have you wondered if we had infinite resources, infinite data, perfect training algorithms with no errors (aka ideal models), can we use this type of model for everything aka 'General AI'? Someone with help from the Beijing Academy of AI used <a href="https://proceedings.mlr.press/v202/yuan23b.html">category theory</a> to model this scenario to see what is possible. The multimodal content here is interesting. 
</p>
</div>
</div>
<div id="outline-container-org62d1402" class="outline-2">
<h2 id="org62d1402">Curriculum</h2>
<div class="outline-text-2" id="text-org62d1402">
<p>
Most of the work is programming in (free) google colab notebooks in C++ and Python though since we are building everything from scratch you could use the language of your choice. Someone wrote Llama.cpp why not write your own too.
</p>
</div>
<div id="outline-container-org01b0b52" class="outline-3">
<h3 id="org01b0b52">What we want to achieve</h3>
<div class="outline-text-3" id="text-org01b0b52">
<ul class="org-ul">
<li><a href="https://dlsyscourse.org">10-714 DL Algorithms &amp; Implementation</a> (CMU)
<ul class="org-ul">
<li>Build from scratch transformers, RNNs, MLPs, everything</li>
<li>Includes hardware acceleration</li>
</ul></li>
<li><a href="https://www.neelnanda.io/mechanistic-interpretability/getting-started">Mechanistic Interpretability</a> (Neel Nanda@Google DeepMind)
<ul class="org-ul">
<li>Reverse engineering transformers</li>
<li>Core material is <a href="https://github.com/callummcdougall/ARENA_3.0/tree/main">here</a> and 90% programming
<ul class="org-ul">
<li>Chapter <a href="https://arena-chapter0-fundamentals.streamlit.app/">0</a></li>
<li>Chapter <a href="https://arena-chapter1-transformer-interp.streamlit.app/">1</a></li>
<li>Chapter <a href="https://arena-chapter2-rl.streamlit.app/">2</a></li>
<li>Chapter <a href="https://arena-chapter3-llm-evals.streamlit.app/">3</a></li>
</ul></li>
</ul></li>
</ul>

<p>
<b>Research</b>
</p>

<p>
This is a graduate course on trying to integrate everything into a shared reasoning and representation system (text, audio, video, actions) so it's the final boss of the AI game. Multimodal doesn't require enormous pretraining such as transformer-based models thus can overcome data scarcity. This means we as pleb researchers can modify open source models like this <a href="https://github.com/Open-Finance-Lab/Awesome-MFFMs/">finance</a> multimodal foundation model. 
</p>

<ul class="org-ul">
<li><a href="https://cmu-mmml.github.io/">Multimodal Machine Learning</a> (CMU)
<ul class="org-ul">
<li>We have access to 2023 <a href="https://www.youtube.com/watch?v=DPkwjgaRvyI&amp;list=PL-Fhd_vrvisMYs8A5j7sj8YW1wHhoJSmW&amp;index=1">lectures</a> but recent lecture structure remains the same</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org326bece" class="outline-3">
<h3 id="org326bece">How to get there</h3>
<div class="outline-text-3" id="text-org326bece">
</div>
<div id="outline-container-org81cb456" class="outline-4">
<h4 id="org81cb456">Intro to machine learning</h4>
<div class="outline-text-4" id="text-org81cb456">
<p>
Every school has an undergrad introduction to machine learning and these courses don't really change from year to year as the underlying theory remains the same.
</p>

<p>
See CMU's latest <a href="https://www.cs.cmu.edu/~mgormley/courses/10601/schedule.html">Intro to Machine Learning</a> (2025) and compare that with the chapters from this 2014 <a href="https://www.cs.cmu.edu/~nihars/teaching/10715-Fa23/index.html">book</a> on the theory of ML. K-nearest neighbors, perceptron (linear predictors), gradient descent, feature engineering, optimization (convex and non-convex), decision trees, PAC learning (probably approximately correct), loss functions, boosting, random forests, neural networks, dimensionality reduction, it's all in that 2014 book because that is the mathematical model of machine learning. If you want the CMU undergrad version you can go back in time using any university's <a href="https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%22892c28d3-f548-4a7a-9de0-ae90011552fa%22">panopto</a> (Summer 2022) recordings. 
</p>

<p>
The <a href="https://www.cs.cmu.edu/~nihars/teaching/10715-Fa23/index.html">graduate</a> versions of introductory machine learning still use that 2014 book so we may as well. Lucky for us the author of that book made a series of lectures for his course at Waterloo and it's a very unusual course he walks through all the math assuming little background.     
</p>

<ul class="org-ul">
<li><a href="https://student.cs.uwaterloo.ca/~cs485/">CS 485 Theory of ML</a> (Waterloo)   
<ul class="org-ul">
<li>All the <a href="https://www.youtube.com/playlist?list=PLt6ES1TJ1gtsvVE_jD-nYaxB2yJzk4zNB">lectures</a> on YouTube</li>
<li>Complete mathematical walkthrough of ML and algorithms</li>
<li>Timeless material as core theory hasn't changed</li>
<li>Matches with Kilian Weinberger's <a href="https://www.youtube.com/playlist?list=PLl8OlHZGYOQ7bkVbuRthEsaLr7bONzbXS">lectures</a> too</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org114a723" class="outline-4">
<h4 id="org114a723">Neural Networks</h4>
<div class="outline-text-4" id="text-org114a723">
<p>
This is to understand the neural networks in the interpretability courses we do and the CMU course on building our own libraries. 
</p>

<ul class="org-ul">
<li><a href="https://jorchard.github.io/cs479.github.io/index.html">CS 479 Neural Networks</a> (Waterloo)
<ul class="org-ul">
<li>Survey of modern NN from the perspective of theoretical neuroscience</li>
<li>We also take <a href="https://youtu.be/VMj-3S1tku0?si=1xkNo1BQ1Y_FcjbS">Building Micrograd</a> by Andrej Karpathy</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orge0cf68a" class="outline-3">
<h3 id="orge0cf68a">Math we need</h3>
<div class="outline-text-3" id="text-orge0cf68a">
<p>
Anything we come across is self-directed research there's no need to take entire math courses except for this one course which is a short 6 week course: 
</p>

<ul class="org-ul">
<li><a href="https://ocw.mit.edu/courses/18-s096-matrix-calculus-for-machine-learning-and-beyond-january-iap-2023/">Matrix Calculus</a> (MIT)
<ul class="org-ul">
<li>All the calculus we need generalized to higher dimensions</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org1707a66" class="outline-2">
<h2 id="org1707a66">Day 1 Scalar Calculus</h2>
<div class="outline-text-2" id="text-org1707a66">
<p>
I thought about including many calc I and II resources here but you won't do them because I won't either. So let's just look it up as we go. 
</p>

<p>
If you super zoom a curve it looks like a straight line. The earth is round but when you're on a boat in the middle of the ocean everything looks flat. That's how super zoomed you are a tiny human on a planet. Take the slope of that seemingly straight line and that's the derivative at that point. We have linearized the curve. By 'super zoom' I mean a very tiny addition input to f(x + 0.0001) you step off the curve by that tiny displacement dx and then dy is the measurement of when you meet the curve/line again. How would you characterize this? In fact it's an <a href="https://classicalrealanalysis.info/documents/Bruckner1995.rae.1341343228.pdf">unsolved</a> problem except the function f(x + displacement) the tiny interval you have created has the characteristics that it's function image is also an interval (called the Darboux property). Beyond that we don't know and the matrix calculus course will show us how curious derivatives are in fact the most badly discontinous function you can imagine probably still has a derivative. The derivative is a higher-order function you turn f(x) = x<sup>2</sup> into f(x) = 2x meaning you get back another function when you 'derive' it from f(x) = x<sup>2</sup>
</p>

<p>
The integral is the reverse operation of derivatives meaning you 'undo' the derivative back to the original function. This is often called the 'fundamental theorem of calculus'. In undergrad they drill every student with the Riemann integral which is honestly a useless integral but because it has a constructive definition (Riemann sums w/limits) you can calculate the area under a curve so it persists as a teaching integral but in graduate school on your first day the Riemann integral is thrown out for the Lebesgue integral and they inform you that 'improper integral's don't actually exist and that your real analysis I and II course you took was actually a waste of time.
</p>

<p>
There is a <a href="https://www.researchgate.net/publication/261629948_The_natural_integral_on_the_real_line">"natural integral"</a> which is the most generalized integral and includes the Lebesgue and the Riemann integral as a special case. This integral is sometimes called the 'gauge integral' or the Henstock-Kurzweil integral who were two mathematicians who discovered it at the same time and then gave a presentation in the 1960s that the Lebesgue integral was 'dead' (not entirely the whole scaffolding of measure theory still persists for probability and many other critical uses). 
</p>

<p>
This integral is taught in <a href="https://www.researchgate.net/publication/261629948_The_natural_integral_on_the_real_line">The Calculus Integral</a> by Brain S. Thomson (intro/elementary version) or in the <a href="http://classicalrealanalysis.info/documents/toti-screen-June11-2013.pdf">Theory of the Integral</a> by the same author or in the <a href="https://classicalrealanalysis.info/documents/TBB-DRIPPED-AllChapters-Landscape.pdf">DRIPPED</a> (drop the Reimann integral) versions of their real analysis text and the non-dripped version is used in MIT's 18.001b <a href="https://ocw.mit.edu/courses/18-100b-real-analysis-spring-2025/pages/readings/">here</a> as all 3 authors are living experts on real analysis. Andrew Bruckner is in his 90s and still writing research papers in 2025. Their elementary real analysis book (esp the <a href="https://classicalrealanalysis.info/documents/TBB-DRIPPED-AllChapters-Landscape.pdf">dripped version</a>) is the single best real analysis text you'll ever find as it contains every test of continuity, every definition of the derivative, every limit definition ect.  
</p>

<p>
If you want a good crash course besides the videos of 3Brown1Blue on YouTube look at <a href="https://www.youtube.com/watch?v=zfHEkt-1sqo">this video</a> by NJ Wildberger he's some radical Yale graduated mathematican who claims the real numbers are a silly abstraction that needs to go but his vids on calculus are excellent. 
</p>
</div>
<div id="outline-container-org75f016d" class="outline-3">
<h3 id="org75f016d">Day 1 task</h3>
<div class="outline-text-3" id="text-org75f016d">
<p>
Watching <a href="https://www.youtube.com/watch?v=zfHEkt-1sqo">this</a> because when we take matrix calculus it will refer to this video. We will learn the construction of integrals when it's needed because remember this is a research curriculum we go back when needed and learn everything needed as we go. 
</p>
</div>
</div>
</div>
<div id="outline-container-org0f47043" class="outline-2">
<h2 id="org0f47043">Day 2 Neuroscience Models</h2>
<div class="outline-text-2" id="text-org0f47043">
<p>
Reminder this is 'learn AI from scratch' thus here is neural models from scratch. 
</p>

<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=uBHW5Rs7qsA&amp;feature=youtu.be">What is a Neuron</a> (26m) from <a href="https://jorchard.github.io/cs479.github.io/index.html">CS479</a>.
<ul class="org-ul">
<li>Hodgkin-Huxley Neuron Model</li>
</ul></li>
</ul>

<p>
@6:39 his example of an action potential using a water bucket being filled and dumped is exactly the same intuition of a potential function in a data structures analysis course used for amortized analysis. A dynamic array once filled up to a certain percentage needs to perform a costly action of doubling itself to enable more free space. You take the infrequent cost of doubling and average it over many read/writes to come up with an individual cost of all array operations. There's some differential equations here but he draws out what they mean, we don't have to know them we just need to know this is a non-linear model and everything to do with neurons is electrical. Hz (hertz) is spikes per second. 
</p>

<p>
You can access all these notebooks he's using for the demo <a href="https://github.com/jorchard/cs479.github.io/tree/master/demos">here</a> if interested.
</p>

<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=0FF1Y8triwE&amp;feature=youtu.be">Simpler Neuron Models</a> (28m) from <a href="https://jorchard.github.io/cs479.github.io/lectures.html">CS479</a>.</li>
</ul>

<p>
I don't have a physics background either but he's explaining the dynamics of a membrane well enough we can get the gist of what's going on. The difference between this first model and the previous Hodgkin-Huxley model is we are no longer modeling the spikes only the action potential for a spike to occur and a reset. Reduces complexity. @18:44 even simpler models. @22:20 ReLU and Softmax are introduced. We will later have to write our own Softmax.
</p>



<hr />
<p>
<a href="./index.html">Home</a>
</p>
</div>
</div>
</div>
</body>
</html>
