<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2025-02-25 Tue 20:19 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>AI in 2025</title>
<meta name="author" content="jbh" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<style> body {background-color: #fafad2; max-width: 62.5rem; padding: 1rem; margin: auto;} </style>
</head>
<body>
<div id="content" class="content">
<h1 class="title">AI in 2025</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgcd2366a">Intro</a></li>
<li><a href="#org6df95cb">Curriculum</a>
<ul>
<li><a href="#org325bd56">Linear Algebra options</a></li>
<li><a href="#org56fdd44">Linear Algebra resources I'll do here</a></li>
<li><a href="#org4a6ae08">Calculus resources I'll do here</a></li>
</ul>
</li>
<li><a href="#org67df801">Day 1 Neuroscience Models</a></li>
<li><a href="#org7eee965">Day 2 Applied Calculus</a>
<ul>
<li><a href="#org8e32b0d">1.1 Functions</a></li>
<li><a href="#org3ed9e81">1.2 Multivariable Functions</a></li>
<li><a href="#org151f19e">1.3 Linear Functions</a></li>
</ul>
</li>
</ul>
</div>
</div>
<style>details summary { color: green; }</style>

<div id="outline-container-orgcd2366a" class="outline-2">
<h2 id="orgcd2366a">Intro</h2>
<div class="outline-text-2" id="text-orgcd2366a">
<p>
We will learn the 'full stack' of modern deep learning systems by building our own library from scratch in your language of choice. Neural networks have different architectures just like there is different computer hardware architectures (x86, RISC, ARM). The most popular currently is transformer architecture for large language models like ChatGPT. We'll learn how to reverse engineer these models. 
</p>

<p>
<a href="https://www.neelnanda.io/about">Neel Nanda</a> has helpfully produced a suggested <a href="https://www.neelnanda.io/mechanistic-interpretability/getting-started">curriculum</a> for reverse engineering transformers. He even offers <a href="https://www.matsprogram.org/interpretability">mentorship</a> through the ML Alignment &amp; Theory Scholars program or you can claim bounties for GeoHot's <a href="https://docs.google.com/spreadsheets/d/1WKHbT-7KOgjEawq5h5Ic1qUWzpfAzuD_J06N1JwOCGs/edit?gid=0#gid=0">TinyGrad</a> as a freelancer.
</p>

<p>
I broke this up into days to see how long it will take and they simply represent a block of time in a single day where I worked on something here. You don't have to do this consecutively everyday.
</p>
</div>

<div id="outline-container-org50507c6" class="outline-4">
<h4 id="org50507c6">Many iterations of this</h4>
<div class="outline-text-4" id="text-org50507c6">
<p>
I nuked this curriculum and stalled it dozens of times for the simple reason it was too expensive to get into but now the GPU bubble finally popped letting us plebs access to hardware to train our own models.  
</p>

<p>
I learned the theory from taking Waterloo's CS485 <a href="https://www.youtube.com/playlist?list=PLt6ES1TJ1gtsvVE_jD-nYaxB2yJzk4zNB">here</a> it is the best course for machine learning you will ever take and goes through the entire mathematical model assuming you have zero background by the Israeli author of <a href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/">Understanding Machine Learning</a>. The theory of learning will not change. CMU still uses this book in their PhD track ML <a href="https://www.cs.cmu.edu/~nihars/teaching/10715-Fa23/index.html">course</a>.
</p>
</div>
</div>

<div id="outline-container-orgea2bc17" class="outline-4">
<h4 id="orgea2bc17">Limits of AI</h4>
<div class="outline-text-4" id="text-orgea2bc17">
<p>
Current popular AI is all <a href="https://en.wikipedia.org/wiki/Foundation_model">foundation models</a> like GPT-n, DALL-E etc. Have you wondered if we had infinite resources, infinite data, perfect training algorithms with no errors, can we use this type of model for everything aka 'General AI'? Someone with help from the Beijing Academy of AI used <a href="https://proceedings.mlr.press/v202/yuan23b.html">category theory</a> to model this scenario to see what is possible.
</p>
</div>
</div>
</div>

<div id="outline-container-org6df95cb" class="outline-2">
<h2 id="org6df95cb">Curriculum</h2>
<div class="outline-text-2" id="text-org6df95cb">
<ul class="org-ul">
<li><a href="https://dlsyscourse.org/lectures/">10-414 Deep Learning Systems</a> (CMU)
<ul class="org-ul">
<li>Use any language you want</li>
</ul></li>
<li><a href="https://jorchard.github.io/cs479.github.io/index.html">CS 479 Neural Networks</a> (Waterloo)
<ul class="org-ul">
<li>Survey on neural networks from the perspective of theoretical neuroscience</li>
<li>Intuition building while hacking through 10-414</li>
</ul></li>
<li><a href="https://www.neelnanda.io/mechanistic-interpretability/getting-started">Mechanistic Interpretability</a> (Google DeepMind)
<ul class="org-ul">
<li>Neel's curriculum on reversing a trained neural network</li>
</ul></li>
<li><a href="https://ocw.mit.edu/courses/18-s096-matrix-calculus-for-machine-learning-and-beyond-january-iap-2023/">Matrix Calculus</a> (MIT)
<ul class="org-ul">
<li>All the calculus we need generalized to higher dimensions</li>
<li>IAP course or 'Independent Activities Period' where faculty can run a 4-week course</li>
</ul></li>
</ul>
</div>

<div id="outline-container-org325bd56" class="outline-3">
<h3 id="org325bd56">Linear Algebra options</h3>
<div class="outline-text-3" id="text-org325bd56">
<p>
There is no shortage of excellent linear algebra courses. These are only what I have reviewed there's tons more if you don't want to do them. 
</p>

<ul class="org-ul">
<li><a href="https://cs.brown.edu/courses/cs053/current/lectures.htm">Coding the Matrix</a> (Brown)
<ul class="org-ul">
<li>All the recorded lectures are open to anyone</li>
<li>Programmed w/Python and has a (nonfree) <a href="https://codingthematrix.com/">book</a>
<ul class="org-ul">
<li>I did this book in OCaml and it worked out fine</li>
</ul></li>
</ul></li>
</ul>

<p>
Created by Philip Klein a name you will recognize if you take any algorithms course. He uses the complex field mostly to illustrate how linear transformations (mappings) work. The SVD is covered which we'll need. The book is at least cheap ($35) and worth buying or you can use Anna's Archive or whatever the latest Library Genesis domain is to get a pdf but it will likely be missing a lot of graphical content. This is a very good course for anyone interested in game graphics or taking an algorithms design course and wants to manipulate graphs using linear algebra. If you hate everything else here then do this you'll be fine.  
</p>

<ul class="org-ul">
<li><a href="https://github.com/mitmath/1806/blob/spring20/summaries.md">Modernized 18.06</a> (MIT)
<ul class="org-ul">
<li>Alan Edelman's course that teaches intuition</li>
<li>No pivots, no echelon forms, no free variables, no hand computation</li>
<li>Treats the SVD as it's own independent thing not some Eigenwhatever</li>
<li>Almost entirely programmed</li>
<li>Lectures got locked up by MIT logins but were once open</li>
<li>All other materials like solutions to homework/recitations are open</li>
</ul></li>
</ul>

<p>
Strang had a stranglehold on MIT's linear algebra curriculum for decades and Edelman came along and shredded it. Now that Strang is retired maybe we'll see the new 18.06 lectures soon. I simply can't follow any book or lecture by Strang it's just disjointed rambling to me.  
</p>

<ul class="org-ul">
<li><a href="https://linear.axler.net/">Linear Algebra Done Right</a> - Sheldon Axler
<ul class="org-ul">
<li>New completely free 4th version</li>
<li>He upgraded his cat too that was always in the about author pic</li>
<li>Abstract treatment which we need but is considered a second course</li>
<li>Contains some calculus</li>
<li>Seems designed to prepare students for functional analysis (and thus ML)</li>
</ul></li>
</ul>


<p>
He banishes determinants to the end of the book for a good reason all laid out in a paper you can find on arxiv. It's great and the proofs are not difficult the way he writes them are clear and intuitive. He doesn't explain what is going on though at all however there is some short YouTube geometry examples he made for the book but then again this is a 'second course' so he doesn't need to. Every mathematician will tell you <i>that is how you learn</i> meaning figure it out yourself and piece it all together seeing that function composition is non-associative just like matrix multiplication ergo a matrix is some kind of encoding of a function (mapping). There's no solutions either but you don't need them you guys can just write any proof you want who cares you learn by going back to the material and digging through it trying to write some argument to yourself why something is true.
</p>

<ul class="org-ul">
<li><a href="https://terrytao.wordpress.com/wp-content/uploads/2016/12/linear-algebra-notes.pdf">Terence Tao's Notes</a> for <a href="https://www.math.ucla.edu/~tao/resource/general/115a.3.02f/">Math 115A</a>
<ul class="org-ul">
<li>The best from scratch linear algebra notes</li>
</ul></li>
</ul>

<p>
These notes were his lectures to accompany the book by Friedberg, Insel and Spence. CMU uses the David Lay book. A lot of other universities like Stanford use <a href="https://web.stanford.edu/~boyd/vmls/">VMLS</a> (free). VMLS also has a Julia language companion. 
</p>


<ul class="org-ul">
<li><a href="http://www.math.clemson.edu/~macaule/classes/s21_math8530/">Math 8530</a> Graduate Linear Algebra (Clemson)
<ul class="org-ul">
<li>Theory of vector spaces not abstract modules</li>
<li>Loosely follows Peter Lax's book chapters 1-9 and Halmos' book on finite-dimensional vector spaces
<ul class="org-ul">
<li>Edelman took this in highschool</li>
</ul></li>
</ul></li>
</ul>

<p>
We could take this. It's highly abstract but not impossibly abstract like replacing scalars in a vector space with entire rings like module theory. Peter Lax when he wrote this book was the foremost expert on computational partial differential equations. PDEs are now being 'solved' by neural networks so there is a quiet scientific field going right now where physics informed machine learning injects physics knowledge to neural networks to supercharge semi-supervised learning. This is where all the early revolutionary AI will come from when we can accelerate training.   
</p>

<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=ndWqKOJ9DgQ">Geometric Linear Algebra</a> NJ Wildberger
<ul class="org-ul">
<li>Tells you what is actually going on</li>
<li>Functionals are covered which we'll need for Matrix calculus</li>
</ul></li>
</ul>

<p>
We will do this but with one of the above resources.     
</p>

<ul class="org-ul">
<li><a href="http://matrixeditions.com/5thUnifiedApproach.html">Hubbard &amp; Hubbard</a> book on unified algebra</li>
</ul>

<p>
Have you ever wanted to redo everything from scratch using linear algebra? There's a book for that. 
</p>
</div>
</div>

<div id="outline-container-org56fdd44" class="outline-3">
<h3 id="org56fdd44">Linear Algebra resources I'll do here</h3>
<div class="outline-text-3" id="text-org56fdd44">
<ul class="org-ul">
<li>Axler's latest <a href="https://linear.axler.net/">book</a>
<ul class="org-ul">
<li>We have no choice but to take this for higher-dimensional ML</li>
</ul></li>
</ul>

<p>
These are optional you can pick something else to gain intuition    
</p>

<ul class="org-ul">
<li>Wildberger's intuitive <a href="https://www.youtube.com/watch?v=yAb12PWrhV0">lectures</a> to figure out Axler</li>
<li>Poh-Shen Loh's <a href="https://www.youtube.com/watch?v=5__JFBncHAA">Matrices</a> Teacher Professional Development</li>
<li>Poh-Shen Loh's <a href="https://www.youtube.com/watch?v=KfiCOUIvjE8">Determinant</a> Teacher Professional Development</li>
<li>Poh-Shen Loh's <a href="https://www.youtube.com/watch?v=uWU0j98jBRo">Putnam Seminar</a> on Linear Algebra</li>
</ul>

<p>
Watching Poh-Shen Loh explain why Matrix multiplication works is amazing. He's definitely my favorite mathematican because he was just a regular guy who failed linear algebra the first time he took it then went on to become to coach of the US olympiad team after thinking they were going to toss him his second year. He survived by going for broke and martingaling on cruise control (making heavy bets to get yourself out of a debt hole) where he decided to prove everything in the olympiad seminar himself in the most creative way possible a huge gamble that paid off.
</p>
</div>
</div>

<div id="outline-container-org4a6ae08" class="outline-3">
<h3 id="org4a6ae08">Calculus resources I'll do here</h3>
<div class="outline-text-3" id="text-org4a6ae08">
<p>
We need to know basic applied math modeling. We could take MIT's calculus sequence on Open CourseWare but that will take at least 6 months and assumes you are an MIT student so already have the prerequisite highschool background. 
</p>

<ul class="org-ul">
<li><a href="https://global.oup.com/academic/product/mathematical-modeling-and-applied-calculus-9780198824725">Mathematical Modeling and Applied Calculus</a> by Kilty &amp; McAllister
<ul class="org-ul">
<li>Use Anna's Archive or lastest Library Genesis domain to get a pdf or buy it and give it to someone after</li>
<li>Partial derivatives, vectors, it's all here</li>
<li>Uses RStudio with a GPL licensed package <a href="https://cran.r-project.org/web/packages/MMAC/index.html">here</a></li>
</ul></li>
</ul>

<p>
The Matrix calculus seminar we take will fill in the rest.   
</p>
</div>
</div>
</div>


<div id="outline-container-org67df801" class="outline-2">
<h2 id="org67df801">Day 1 Neuroscience Models</h2>
<div class="outline-text-2" id="text-org67df801">
<p>
This is 'learn AI from scratch' so that's what we're going to do starting with brain models. I'm including this here in the beginning so we can see what kind of math models we need to teach ourselves. 
</p>

<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=uBHW5Rs7qsA&amp;feature=youtu.be">What is a Neuron</a> (26m) from <a href="https://jorchard.github.io/cs479.github.io/index.html">CS479</a>.
<ul class="org-ul">
<li>Hodgkin-Huxley Neuron Model</li>
</ul></li>
</ul>

<p>
If we're going to model a brain with a neural network we should see how it works in the real world. @6:39 his example of an action potential using a water bucket being filled and dumped is exactly the same intuition of a potential function in a data structures analysis course used for amortized analysis. A dynamic array once filled up to a certain percentage needs to perform a costly action of doubling itself to enable more free space. You take the infrequent cost of doubling and average it over many read/writes to come up with an individual cost of all array operations. There's some differential equations here but he draws out what they mean, we don't have to know them we just need to know this is a non-linear model and everything to do with neurons is electrical. Hz (hertz) is spikes per second. 
</p>

<p>
You can access all these notebooks he's using for the demo <a href="https://github.com/jorchard/cs479.github.io/tree/master/demos">here</a> if interested.
</p>

<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=0FF1Y8triwE&amp;feature=youtu.be">Simpler Neuron Models</a> (28m) from <a href="https://jorchard.github.io/cs479.github.io/lectures.html">CS479</a>.</li>
</ul>

<p>
I don't have a physics background either but he's explaining the dynamics of a membrane well enough we can get the gist of what's going on. The difference between this first model and the previous Hodgkin-Huxley model is we are no longer modeling the spikes only the action potential for a spike to occur and a reset. Reduces complexity. @18:44 even simpler models. These Sigmoidal models are covered in the calculus book we'll do shortly after this. @22:20 ReLU and SoftMax are introduced. 
</p>
</div>
</div>

<div id="outline-container-org7eee965" class="outline-2">
<h2 id="org7eee965">Day 2 Applied Calculus</h2>
<div class="outline-text-2" id="text-org7eee965">
<p>
As mentioned above obtain the book <i>Mathematical Modeling and Applied Calculus</i> by Joel Kilty and Alex McAllister either buy it, borrow it, or download it from <a href="https://annas-archive.org/">Anna's Archive</a> or a working <a href="https://en.wikipedia.org/wiki/Library_Genesis">Library Genesis</a> domain. All the simplified neuron models we just watched in the CS479 lectures are in this book and there's an RStudio <a href="https://cran.r-project.org/web/packages/MMAC/index.html">package</a> to do the exercises with. You can also directly interface with any R library using <a href="https://cran.r-project.org/web/packages/JuliaCall/readme/README.html">JuliaCall</a>. 
</p>
</div>

<div id="outline-container-org8e32b0d" class="outline-3">
<h3 id="org8e32b0d">1.1 Functions</h3>
<div class="outline-text-3" id="text-org8e32b0d">
<p>
It seems everyone already changed the name of a range to co-domain since this was written. If you look at page 770 <i>Appendix C</i> the instructions how to install RStudio and the book packages are there. Took me about 20 minutes using an Ubuntu VM as the 'mosaic' package does a bunch of c++ compilations. I had to manually install libudunits2-dev and libgdal-dev to get the R package 'mosaicCalc' to install. Project <a href="https://dtkaplan.github.io/SM2-bookdown/">mosaic</a> now has their own calculus book but the one we are doing is more advanced so we'll keep doing it.
</p>

<p>
Once you get RStudio working and import/require the libraries all the plotFun commands work.
</p>
</div>
</div>

<div id="outline-container-org3ed9e81" class="outline-3">
<h3 id="org3ed9e81">1.2 Multivariable Functions</h3>
<div class="outline-text-3" id="text-org3ed9e81">
<p>
3blue1brown got his start making Khan Academy videos and here is his <a href="https://youtu.be/WsZj5Rb6do8?si=7LHO8h0V0DA6uNYJ">video</a> explaining a contour plot. 
</p>
</div>
</div>

<div id="outline-container-org151f19e" class="outline-3">
<h3 id="org151f19e">1.3 Linear Functions</h3>
<div class="outline-text-3" id="text-org151f19e">
<p>
The book will now describe everything in terms of a model with parameters.
</p>

<p>
TODO
</p>

<hr />
<p>
<a href="./index.html">Home</a>
</p>
</div>
</div>
</div>
</div>
</body>
</html>
